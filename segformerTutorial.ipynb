{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Install pytorch! ",
   "id": "b39171619a802ae1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:17.347522Z",
     "start_time": "2024-06-20T15:38:15.216727Z"
    }
   },
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, image_processor, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegFormerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "            train (bool): Whether to load \"training\" or \"validation\" images + annotations.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = image_processor\n",
    "        self.train = train\n",
    "\n",
    "        sub_path = \"training\" if self.train else \"validation\"\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\", sub_path)\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"annotations\", sub_path)\n",
    "\n",
    "        # read images\n",
    "        image_file_names = []\n",
    "        for root, dirs, files in os.walk(self.img_dir):\n",
    "          image_file_names.extend(files)\n",
    "        self.images = sorted(image_file_names)\n",
    "\n",
    "        # read annotations\n",
    "        annotation_file_names = []\n",
    "        for root, dirs, files in os.walk(self.ann_dir):\n",
    "          annotation_file_names.extend(files)\n",
    "        self.annotations = sorted(annotation_file_names)\n",
    "\n",
    "        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = np.array(Image.open(os.path.join(self.img_dir, self.images[idx])))\n",
    "        segmentation_map = np.array(Image.open(os.path.join(self.ann_dir, self.annotations[idx])).convert(\"L\"))\n",
    "        \n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:18.048383Z",
     "start_time": "2024-06-20T15:38:17.350530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import SegformerImageProcessor\n",
    "\n",
    "# my_root_dir = \".content/ADE20k_toy_dataset\"\n",
    "my_root_dir = \".content/customDatasetWithLabel\"\n",
    "my_image_processor = SegformerImageProcessor(do_reduce_labels=True) #changed reduce_labels to do_reduce_labels\n",
    "\n",
    "train_dataset = SemanticSegmentationDataset(root_dir=my_root_dir, image_processor=my_image_processor)\n",
    "valid_dataset = SemanticSegmentationDataset(root_dir=my_root_dir, image_processor=my_image_processor, train=False)"
   ],
   "id": "a21a5342a1d8c2fe",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:18.064226Z",
     "start_time": "2024-06-20T15:38:18.050249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(valid_dataset))\n"
   ],
   "id": "be7d643e81bdbd74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 427\n",
      "Number of validation examples: 0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:18.110441Z",
     "start_time": "2024-06-20T15:38:18.066126Z"
    }
   },
   "cell_type": "code",
   "source": "my_encoded_inputs= train_dataset[0]",
   "id": "28d4167efc5ae661",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:18.125571Z",
     "start_time": "2024-06-20T15:38:18.112441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_encoded_inputs[\"pixel_values\"].shape\n",
    "    "
   ],
   "id": "f74922b5642c2906",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:18.140956Z",
     "start_time": "2024-06-20T15:38:18.126713Z"
    }
   },
   "cell_type": "code",
   "source": "my_encoded_inputs[\"labels\"].shape\n",
   "id": "4e3c96d35157034f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:18.156152Z",
     "start_time": "2024-06-20T15:38:18.142935Z"
    }
   },
   "cell_type": "code",
   "source": "my_encoded_inputs[\"labels\"]\n",
   "id": "73616faa10064788",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[255, 255, 255,  ...,   7,   7,   7],\n",
       "        [  1,   1,   1,  ...,   7,   7,   7],\n",
       "        [  1,   1,   1,  ...,   7,   7,   7],\n",
       "        ...,\n",
       "        [  1,   1,   1,  ..., 255, 255, 255],\n",
       "        [  1,   1,   1,  ..., 255, 255, 255],\n",
       "        [  1,   1,   1,  ..., 255, 255, 255]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:18.172047Z",
     "start_time": "2024-06-20T15:38:18.157327Z"
    }
   },
   "cell_type": "code",
   "source": "my_encoded_inputs[\"labels\"].squeeze().unique() #to see unique ids of labels in an image",
   "id": "e8e0873907070b89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   2,   3,   7, 255])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:18.187326Z",
     "start_time": "2024-06-20T15:38:18.173045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=2)\n"
   ],
   "id": "c8ee70608afe6aa5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:18.249244Z",
     "start_time": "2024-06-20T15:38:18.190457Z"
    }
   },
   "cell_type": "code",
   "source": "batch = next(iter(train_dataloader))\n",
   "id": "de619b56111b9041",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:18.264935Z",
     "start_time": "2024-06-20T15:38:18.250244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for k2,v2 in batch.items():\n",
    "  print(k2, v2.shape)"
   ],
   "id": "37aab3af7a6ce49a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([2, 3, 512, 512])\n",
      "labels torch.Size([2, 512, 512])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:18.287318Z",
     "start_time": "2024-06-20T15:38:18.266932Z"
    }
   },
   "cell_type": "code",
   "source": "batch[\"labels\"].shape\n",
   "id": "5da8c4461b860967",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:18.302889Z",
     "start_time": "2024-06-20T15:38:18.289319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask = (batch[\"labels\"] != 255)\n",
    "mask\n"
   ],
   "id": "d0f3b36c1d7d6bfd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:18.318352Z",
     "start_time": "2024-06-20T15:38:18.303888Z"
    }
   },
   "cell_type": "code",
   "source": "batch[\"labels\"][mask]\n",
   "id": "bb71574d173900c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 3, 3, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:19.263036Z",
     "start_time": "2024-06-20T15:38:18.320345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# #defining the model and labels. \n",
    "# \n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "import json\n",
    "\n",
    "#from huggingface_hub import hf_hub_download\n",
    "# \n",
    "# # load id2label mapping from a JSON on the hub\n",
    "# repo_id = \"huggingface/label-files\"\n",
    "# filename = \"ade20k-id2label.json\"\n",
    "# id2label = json.load(open(hf_hub_download(repo_id=repo_id, filename=filename, repo_type=\"dataset\"), \"r\"))\n",
    "id2label = {0: 'other', 1: 'Manhole Cover', 2: 'Concrete',3: 'Brick',4: 'Cane', 5: 'Subway grate', 6:'Dirt', 7:'Cellar door', 8:'Tactile pavement'}\n",
    "\n",
    "with open('cats-and-dogs-id2label.json', 'w') as fp:\n",
    "    json.dump(id2label, fp)\n",
    "\n",
    "\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    " \n",
    "#define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\",\n",
    "                                                         num_labels=9,\n",
    "                                                         id2label=id2label,\n",
    "                                                         label2id=label2id,\n",
    ")\n",
    "\n",
    "#load id2label mapping\n",
    "\n"
   ],
   "id": "98649331d6b99229",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:38:20.361085Z",
     "start_time": "2024-06-20T15:38:19.265035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"mean_iou\")\n"
   ],
   "id": "6fb88060a2eca482",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-20T15:38:20.362595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#I didn't touch the model. I mean it's a toy data set, obviously. I just wanted to see if it was working, and it did.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "   print(\"Epoch:\", epoch)\n",
    "   for idx2, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluate\n",
    "        with torch.no_grad():\n",
    "          upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "          predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "          # note that the metric expects predictions + labels as numpy arrays\n",
    "          metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "\n",
    "        # let's print loss and metrics every 100 batches\n",
    "        if idx2 % 1 == 0:\n",
    "          # currently using _compute instead of compute\n",
    "          # see this issue for more info: https://github.com/huggingface/evaluate/pull/328#issuecomment-1286866576\n",
    "          \n",
    "          #changed it to compute.\n",
    "            metrics = metric.compute(\n",
    "                  predictions=predicted.cpu(),\n",
    "                  references=labels.cpu(),\n",
    "                  num_labels=len(id2label),\n",
    "                  ignore_index=255,\n",
    "                  reduce_labels=False, # we've already reduced the labels ourselves\n",
    "            )\n",
    "            print(\"Loss:\", loss.item())\n",
    "            print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
    "            print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])"
   ],
   "id": "4fd9706897ec2927",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53cfa37e3ec844e5b7e9d1304a26753f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mehdi\\PycharmProjects\\SegformerTutorial\\.venv\\lib\\site-packages\\datasets\\features\\image.py:348: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
      "C:\\Users\\mehdi\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--mean_iou\\9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0\\mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = total_area_intersect / total_area_label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3217408657073975\n",
      "Mean_iou: 0.005480565531787611\n",
      "Mean accuracy: 0.1620940808928717\n",
      "Loss: 2.2800979614257812\n",
      "Mean_iou: 0.007475974514783758\n",
      "Mean accuracy: 0.10990251853564666\n",
      "Loss: 2.2291507720947266\n",
      "Mean_iou: 0.011648753420787068\n",
      "Mean accuracy: 0.3446529106156488\n",
      "Loss: 2.2076005935668945\n",
      "Mean_iou: 0.027507815086194123\n",
      "Mean accuracy: 0.20186882313080126\n",
      "Loss: 2.1431725025177\n",
      "Mean_iou: 0.02659441648321248\n",
      "Mean accuracy: 0.2718027504882842\n",
      "Loss: 2.1715071201324463\n",
      "Mean_iou: 0.022738004971065085\n",
      "Mean accuracy: 0.22719936666736681\n",
      "Loss: 2.1897592544555664\n",
      "Mean_iou: 0.05119878526695362\n",
      "Mean accuracy: 0.27024921390516066\n",
      "Loss: 2.1101083755493164\n",
      "Mean_iou: 0.04488615652556485\n",
      "Mean accuracy: 0.1038423114961065\n",
      "Loss: 2.2817602157592773\n",
      "Mean_iou: 0.00813490654309268\n",
      "Mean accuracy: 0.3258307175854754\n",
      "Loss: 2.2183821201324463\n",
      "Mean_iou: 0.01474015369948071\n",
      "Mean accuracy: 0.4647211767583036\n",
      "Loss: 2.1190192699432373\n",
      "Mean_iou: 0.05020101810121687\n",
      "Mean accuracy: 0.41132048725636466\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "#inference\n",
    "firstImage = Image.open('.content/customDatasetWithLabel/images/training/0@0_2023-03-23_15-38-19-4410_Brick.jpg')\n",
    "firstImage"
   ],
   "id": "48a914a5b8bdde99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# prepare the image for the model\n",
    "pixel_values = my_image_processor(firstImage, return_tensors=\"pt\").pixel_values.to(device)\n",
    "print(pixel_values.shape)\n"
   ],
   "id": "26a44aa2503a9c14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = model(pixel_values=pixel_values)\n"
   ],
   "id": "6558e177b418f67b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# logits are of shape (batch_size, num_labels, height/4, width/4)\n",
    "logits = outputs.logits.cpu()\n",
    "print(logits.shape)"
   ],
   "id": "b154b9b98e6a881a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def ade_palette():\n",
    "    \"\"\"ADE20K palette that maps each class to RGB values.\"\"\"\n",
    "    #Changed to my own palette cuz why not\n",
    "    return [[1,1,1], [31, 31, 31], [62, 62, 62], [93, 93, 93],\n",
    "            [123, 123, 123], [154, 154, 154], [185,185, 185], [216, 216, 216],\n",
    "            [247, 247, 247]]\n"
   ],
   "id": "1e0ad79c61e9da0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "predicted_segmentation_map = my_image_processor.post_process_semantic_segmentation(outputs, target_sizes=[firstImage.size[::-1]])[0]\n",
    "predicted_segmentation_map = predicted_segmentation_map.cpu().numpy()\n",
    "print(predicted_segmentation_map)\n"
   ],
   "id": "a2bd1c3ba65eddda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_seg = np.zeros((predicted_segmentation_map.shape[0],\n",
    "                      predicted_segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "\n",
    "palette = np.array(ade_palette())\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[predicted_segmentation_map == label, :] = color\n",
    "# Convert to BGR\n",
    "color_seg = color_seg[..., ::-1]\n",
    "\n",
    "# Show image + mask\n",
    "img = np.array(firstImage) * 0.5 + color_seg * 0.5\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ],
   "id": "fde105f0f2c62c59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "firstAnnotation = Image.open('.content/customDatasetWithLabel/annotations/training/0@0_2023-03-23_15-38-19-4410_Brick.png')\n",
    "firstAnnotation\n"
   ],
   "id": "71e6242616d97279",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# convert map to NumPy array\n",
    "firstAnnotation = np.array(firstAnnotation)\n",
    "firstAnnotation[firstAnnotation == 0] = 255 # background class is replaced by ignore_index\n",
    "firstAnnotation = firstAnnotation - 1 # other classes are reduced by one\n",
    "firstAnnotation[firstAnnotation == 254] = 255\n",
    "\n",
    "classes_map = np.unique(firstAnnotation).tolist()\n",
    "unique_classes = [model.config.id2label[idx2] if idx2!=255 else None for idx2 in classes_map]\n",
    "print(\"Classes in this image:\", unique_classes)\n",
    "\n",
    "# create coloured map\n",
    "color_seg = np.zeros((firstAnnotation.shape[0], firstAnnotation.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "palette = np.array(ade_palette())\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[map == label, :] = color\n",
    "# Convert to BGR\n",
    "color_seg = color_seg[..., ::-1]\n",
    "\n",
    "# Show image + mask\n",
    "img = np.array(firstImage) * 0.5 + color_seg * 0.5\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ],
   "id": "a72ea7b6cc21358",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# metric expects a list of numpy arrays for both predictions and references\n",
    "metrics = metric.compute(\n",
    "                  predictions=[predicted_segmentation_map],\n",
    "                  references=[firstAnnotation],\n",
    "                  num_labels=len(id2label),\n",
    "                  ignore_index=255,\n",
    "                  reduce_labels=False, # we've already reduced the labels ourselves\n",
    "              )\n"
   ],
   "id": "ceefc55c0111909a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "metrics.keys()",
   "id": "9232965492f99cef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# print overall metrics\n",
    "for key in list(metrics.keys())[:3]:\n",
    "  print(key, metrics[key])\n",
    "\n",
    "# pretty-print per category metrics as Pandas DataFrame\n",
    "metric_table = dict()\n",
    "for my_id, label in id2label.items():\n",
    "    metric_table[label] = [\n",
    "                           metrics[\"per_category_iou\"][my_id],\n",
    "                           metrics[\"per_category_accuracy\"][my_id]\n",
    "    ]\n",
    "\n",
    "print(\"---------------------\")\n",
    "print(\"per-category metrics:\")\n",
    "pd.DataFrame.from_dict(metric_table, orient=\"index\", columns=[\"IoU\", \"accuracy\"])\n"
   ],
   "id": "3a9b36023a59b0dc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
